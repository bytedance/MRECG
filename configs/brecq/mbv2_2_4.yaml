extra_prepare_dict:
    extra_qconfig_dict:
        w_observer: MSEObserver
        a_observer: EMAMSEObserver
        w_fakequantize: AdaRoundFakeQuantize
        a_fakequantize: QDropFakeQuantize
        w_qscheme:
            bit: 2
            symmetry: False
            per_channel: True
            pot_scale: False
            p: 2.4
        a_qscheme:
            bit: 4
            symmetry: False
            per_channel: False
            pot_scale: False
            p: 2.4
quantize:
    quantize_type: advanced_ptq # support naive_ptq or advanced_ptq
    cali_batchsize: 16
    reconstruction:
        pattern: block
        scale_lr: 4.0e-5
        warm_up: 0.2
        weight: 0.1
        max_count: 20000
        b_range: [20,2]
        keep_gpu: True
        round_mode: learned_hard_sigmoid
        prob: 1.0
        scheme: "[1, 2, (3, 3), 4, (5, 5), 6, (7, 7, 7), (8, 8, 8), 9, (10, 10), 11]"

model:                    # architecture details
    type: mobilenet_v2        # model name
    kwargs:
        num_classes: 1000
        width_mult: 0.5
    path: ./pretrained/mobilenetv2_0.5-eaa6f9ad.pth
data:
    path: /path/to/imagenet
    batch_size: 256
    num_workers: 4
    pin_memory: True
    input_size: 224
    test_resize: 256
process:
    seed: 1005
